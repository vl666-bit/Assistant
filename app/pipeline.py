from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np

from app.utils import save_file  # —É —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å utils.py ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
from app.embedding import get_embeddings
from app.retrieval import (
    store_embeddings,
    query_similar,
    query_similar_in_pages,
)
from app.retrieval_outline import search_outline
from app.llm_stub import generate_answer
from app.db import pages_db

# ===== –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ =====
def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> List[str]:
    """
    –î–µ–ª–∏—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.
    """
    if not text:
        return []
    chunks: List[str] = []
    start = 0
    step = max(1, chunk_size - overlap)
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        start += step
    return chunks

# ===== –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ =====
def process_file(filename: str, content: bytes) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª, —Ä–µ–∂–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏, —Å—á–∏—Ç–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–ª–∞–¥—ë—Ç –≤ –≤–µ–∫—Ç–æ—Ä–∫—É.
    page_id = –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è.
    """
    path: Path = save_file(filename, content)
    try:
        raw_text = path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        raw_text = ""

    texts = chunk_text(raw_text)
    if not texts:
        return f"–§–∞–π–ª {filename} —Å–æ—Ö—Ä–∞–Ω—ë–Ω, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω (0 —á–∞–Ω–∫–æ–≤)."

    embeddings = get_embeddings(texts)            # List[List[float]]
    vectors = np.asarray(embeddings, dtype=np.float32)
    page_id = Path(filename).stem

    store_embeddings(page_id, texts, vectors)
    return f"–§–∞–π–ª {filename} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω ({len(texts)} —á–∞–Ω–∫–æ–≤)."

# ===== –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–æ score/similarity =====
def _best_score(d: Dict[str, Any]) -> Optional[float]:
    if d.get("score") is not None:
        return float(d["score"])
    if d.get("similarity") is not None:
        return float(d["similarity"])
    return None

# ===== –°—Ç–∞—Ä—ã–π –ø—É—Ç—å: –æ–±—â–∏–π ANN –ø–æ –≤—Å–µ–π –±–∞–∑–µ =====
def answer_query(query: str, page_ids: Optional[List[str]] = None, top_k: int = 5) -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {"answer": str, "sources": list[dict], "chunks": list[dict]}
    """
    print(f"\nüü¢ answer_query: query='{query}', page_ids={page_ids}, top_k={top_k}")

    qv = np.asarray(get_embeddings([query])[0], dtype=np.float32)
    hits = query_similar(qv, top_k=top_k, page_ids=page_ids)

    if not hits:
        return {"answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.", "sources": [], "chunks": []}

    # –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
    context = "\n\n---\n\n".join([h["document"] for h in hits if h.get("document")])

    # –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources, seen = [], set()
    for h in hits:
        pid = h.get("page_id")
        if pid and pid not in seen:
            page = pages_db.get_page(pid)
            sources.append({"page_id": pid, "title": (page["title"] if page else None)})
            seen.add(pid)

    # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Äî –ø—Ä–æ–º–ø—Ç –≤–Ω—É—Ç—Ä–∏ llm_stub
    answer = generate_answer(query, context)
    return {"answer": answer, "sources": sources, "chunks": hits}

# ===== –ù–æ–≤—ã–π –ø—É—Ç—å: outline ‚Üí –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ‚Üí ANN –ø–æ –Ω–∏–º =====
def _format_short_history(history: Optional[List[Dict[str, str]]], limit_pairs: int = 4) -> str:
    """
    –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞:
    history: [{"role": "user"|"assistant", "text": "..."}]
    –ë–µ—Ä—ë–º –¥–æ 4 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–∞—Ä (–º–∞–∫—Å 8 —Å–æ–æ–±—â–µ–Ω–∏–π).
    """
    if not history:
        return ""
    last = history[-(limit_pairs * 2):]
    return "\n".join(f"{m.get('role')}: {m.get('text')}" for m in last if m.get("text"))

def answer_query_via_outline(
    query: str,
    *,
    top_nodes: int = 12,
    top_pages: int = 6,
    top_k: int = 12,
    chat_history: Optional[List[Dict[str, str]]] = None
) -> Dict[str, Any]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é -> –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º -> –ø–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö.
    –¢—Ä–µ–±—É–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π outline (OutlinePipeline.build_outline()).
    """
    print(f"\nüü¢ answer_query_via_outline: '{query}' | top_nodes={top_nodes}, top_pages={top_pages}, top_k={top_k}")

    # 1) —Å—É–∑–∏—Ç—å –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é
    candidate_pages: List[str] = search_outline(query, top_nodes=top_nodes, top_pages=top_pages) or []
    candidate_pages = [pid for pid in dict.fromkeys(candidate_pages) if pid]
    print(f"   ‚Üí —Å—Ç—Ä–∞–Ω–∏—Ü—ã-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã: {candidate_pages}")

    if not candidate_pages:
        return {"answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é).", "sources": [], "chunks": []}

    # 2) –ø–æ–∏—Å–∫ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
    qv = np.asarray(get_embeddings([query])[0], dtype=np.float32)
    hits = query_similar_in_pages(qv, page_ids=candidate_pages, top_k=top_k) or []
    if not hits:
        return {"answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–ø–æ—Å–ª–µ —Å—É–∂–µ–Ω–∏—è –ø–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é).", "sources": [], "chunks": []}

    # 3) –∫–æ–Ω—Ç–µ–∫—Å—Ç (–∏—Å—Ç–æ—Ä–∏—è + —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã)
    short_ctx = _format_short_history(chat_history, limit_pairs=4)
    chunks_ctx = "\n\n---\n\n".join([h["document"] for h in hits if h.get("document")])
    merged_context = (f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{short_ctx}\n\n" if short_ctx else "") + \
                     f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:\n{chunks_ctx}"

    # 4) –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources, seen = [], set()
    for h in hits:
        pid = h.get("page_id")
        if pid and pid not in seen:
            page = pages_db.get_page(pid)
            sources.append({"page_id": pid, "title": (page["title"] if page else None)})
            seen.add(pid)

    # 5) –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    answer = generate_answer(query, merged_context)
    return {"answer": answer, "sources": sources, "chunks": hits}
