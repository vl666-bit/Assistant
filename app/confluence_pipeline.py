# app/confluence_pipeline.py
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from collections import deque

from app.connectors.confluence_connector import ConfluenceConnector
from app.embedding import get_embeddings
from app.retrieval import store_embeddings, query_similar_in_pages
from app.llm_stub import generate_answer
from app.pipeline import chunk_text
from app.db import pages_db, chunks_db
from app.retrieval_outline import search_outline
from app.utils import now_ts, uuid4_str, sha256_text
from config import CONFLUENCE


# --- helpers: Confluence URLs ---

def _get_confluence_base_url() -> str:
    """
    Ğ”Ğ¾ÑÑ‚Ğ°Ñ‘Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ URL Ğ¸Ğ· ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ°, Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°Ñ.
    Ğ”Ğ»Ñ Atlassian Cloud Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ /wiki, ĞµÑĞ»Ğ¸ ĞµĞ³Ğ¾ Ğ½ĞµÑ‚.
    """
    cfg = CONFLUENCE or {}
    base = (
        cfg.get("base_url")
        or cfg.get("BASE_URL")
        or cfg.get("url")
        or cfg.get("URL")
        or cfg.get("domain")
        or cfg.get("DOMAIN")
        or ""
    )
    base = (base or "").rstrip("/")
    if "atlassian.net" in base and not base.endswith("/wiki"):
        base = base + "/wiki"
    return base


def confluence_page_url(page_id: str) -> Optional[str]:
    base = _get_confluence_base_url()
    if not base:
        return None
    return f"{base}/pages/viewpage.action?pageId={page_id}"


# ========= Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞĞ• =========

def _ins_page_meta(
    *,
    page_id: str,
    project_id: str,
    title: str,
    parent_id: Optional[str],
    version: Optional[str] = None,
    last_modified: Optional[str] = None,
) -> None:
    """
    ĞĞ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞĞ• ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ parent_id (NULL), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ»Ğ¾Ğ²Ğ¸Ñ‚ÑŒ FK-Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸.
    parent_id Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ²Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ²ÑĞµÑ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†.
    """
    pages_db.insert_page(
        page_id=page_id,
        project_id=project_id,
        title=title,
        parent_id=None,
        last_modified=(last_modified or (version or "")),
        content=None,
        url=confluence_page_url(page_id),  # â† ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºÑƒ ÑÑ€Ğ°Ğ·Ñƒ
    )


# ========= ĞĞ¡ĞĞĞ’ĞĞĞ™ ĞŸĞĞ™ĞŸ =========

class ConfluencePipeline:
    def __init__(self, domain: str, email: str, api_token: str):
        self.connector = ConfluenceConnector(domain, email, api_token)

    # ===== INIT: Ğ¿Ğ¾Ğ´Ñ‚ÑĞ½ÑƒÑ‚ÑŒ Ğ’Ğ¡Ğ• Ğ²ĞµÑ‚Ğ²Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° (2 Ñ„Ğ°Ğ·Ñ‹ Ğ´Ğ»Ñ FK) =====
    def init_structure(self, per_space_limit: int = 5000) -> Dict[str, Any]:
        """
        ĞĞ° ÑÑ‚Ğ°Ñ€Ñ‚Ğµ:
        1) ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ spaces.
        2) Ğ’ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† + Ğ¿Ğ°Ñ€Ñ‹ (child -> parent).
           Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ get_pages_meta; ĞµÑĞ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾ â€” BFS Ğ¿Ğ¾ get_pages_in_space + get_child_pages.
        3) Ğ¤Ğ°Ğ·Ğ° 1 â€” Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ’Ğ¡Ğ• ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ‘Ğ•Ğ— parent_id (NULL), Ğ½Ğ¾ Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¼ url.
        4) Ğ¤Ğ°Ğ·Ğ° 2 â€” Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ²Ñ‹ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ parent_id Ñ‚ĞµĞ¼, Ñƒ ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ ĞµÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ….
        """
        spaces = self.connector.get_spaces()
        print(f"ğŸ”„ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²: {len(spaces)}")

        total_inserted = 0
        total_parents_set = 0
        spaces_stats: List[Dict[str, Any]] = []

        for sp in spaces:
            sid = sp["id"]
            sname = sp.get("name") or sp.get("key") or sid
            pages_db.insert_project(sid, sname)

            # --- ÑĞ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ meta Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ ---
            all_pages: List[Dict[str, Any]] = []              # [{"id","title"}]
            parent_links: List[Tuple[str, Optional[str]]] = [] # [(child_id, parent_id)]
            meta_pages: List[Dict[str, Any]] = []

            try:
                meta_pages = self.connector.get_pages_meta(sid, limit=per_space_limit) or []
            except Exception as e:
                print(f"âš ï¸ get_pages_meta failed for space {sid}: {e}")

            if meta_pages:
                for p in meta_pages:
                    pid = p.get("id")
                    if not pid:
                        continue
                    all_pages.append({"id": pid, "title": p.get("title") or pid})
                    parent_links.append((pid, p.get("parent_id")))
            else:
                # --- fallback: BFS Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ ---
                print(f"â†ªï¸ Fallback BFS Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° {sid}")
                roots = self.connector.get_pages_in_space(sid, limit=200) or []
                seen = set()
                q = deque()

                for r in roots:
                    pid = r.get("id")
                    if not pid or pid in seen:
                        continue
                    seen.add(pid)
                    all_pages.append({"id": pid, "title": r.get("title") or pid})
                    parent_links.append((pid, None))
                    q.append(pid)

                while q:
                    cur = q.popleft()
                    try:
                        children = self.connector.get_child_pages(cur) or []
                    except Exception as e:
                        print(f"âš ï¸ get_child_pages failed for page {cur}: {e}")
                        children = []
                    for ch in children:
                        cid = ch.get("id")
                        if not cid or cid in seen:
                            continue
                        seen.add(cid)
                        all_pages.append({"id": cid, "title": ch.get("title") or cid})
                        parent_links.append((cid, cur))
                        q.append(cid)

            # --- Ğ¤Ğ°Ğ·Ğ° 1: Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ²ÑĞµÑ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ±ĞµĞ· parent_id, Ğ½Ğ¾ Ñ url ---
            inserted_here = 0
            for p in all_pages:
                pid = p["id"]
                pages_db.insert_page(
                    page_id=pid,
                    project_id=sid,
                    title=p.get("title") or pid,
                    parent_id=None,
                    last_modified=None,
                    content=None,
                    url=confluence_page_url(pid),  # â† ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑÑ‹Ğ»ĞºÑƒ
                )
                inserted_here += 1

            # --- Ğ¤Ğ°Ğ·Ğ° 2: Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ parent_id (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ ĞµÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ…) ---
            ids_in_space = {p["id"] for p in all_pages}
            parents_set = 0
            for child_id, parent_id in parent_links:
                if parent_id and parent_id in ids_in_space:
                    pages_db.update_parent_id(child_id, parent_id)
                    parents_set += 1

            total_inserted += inserted_here
            total_parents_set += parents_set
            spaces_stats.append({
                "space_id": sid,
                "inserted": inserted_here,
                "parents_set": parents_set
            })

        print(f"âœ… Ğ’ÑĞµĞ³Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (Ğ¼ĞµÑ‚Ğ°) Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ² Ğ‘Ğ”: {total_inserted}; parent_id Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾: {total_parents_set}")
        return {
            "spaces": len(spaces),
            "pages_meta_inserted": total_inserted,
            "parents_set": total_parents_set,
            "per_space": spaces_stats
        }

    def refresh_structure(self, per_space_limit: int = 5000) -> Dict[str, Any]:
        print("ğŸ”„ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Confluence (Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°)...")
        return self.init_structure(per_space_limit=per_space_limit)

    # ===== Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ (on-demand), Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ â€” Ñ Ğ´ĞµÑ‚ÑŒĞ¼Ğ¸ =====
    def index_page(self, page_id: str, include_children: bool = True):
        def _index_one(pid: str):
            page = self.connector.get_page(pid, with_content=True)
            text = page.get("content_text", "") if page else ""
            if not text:
                return
            chunks = chunk_text(text)
            embeddings = get_embeddings(chunks)

            # upsert Ğ² Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ (Chroma)
            store_embeddings(pid, chunks, embeddings)

            # Ğ°Ğ¿Ğ´ĞµĞ¹Ñ‚ Ğ¼ĞµÑ‚Ñ‹ (ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ space/parent/title/url)
            existing = pages_db.get_page(pid)
            pages_db.insert_page(
                page_id=pid,
                project_id=(existing["project_id"] if existing else page.get("space_id") or "?"),
                title=page.get("title") or (existing["title"] if existing else pid),
                parent_id=(existing["parent_id"] if existing else page.get("parent_id")),
                content=text,
                url=(existing.get("url") if existing else None) or confluence_page_url(pid),  # â† Ğ½Ğµ Ñ‚ĞµÑ€ÑĞµĞ¼ url
            )

            # Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ğ´ĞµĞºÑ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² (ĞµÑĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ)
            for i, emb in enumerate(embeddings):
                chunks_db.insert_chunk(pid, i, chunks[i], np.asarray(emb, dtype=np.float32).tobytes())

            print(f"ğŸ“‘ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° '{page.get('title', pid)}' Ğ¿Ñ€Ğ¾Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ({len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²).")

        _index_one(page_id)
        if include_children:
            for ch in (pages_db.get_child_pages(page_id) or []):
                _index_one(ch["id"])

    # ===== Ğ•Ğ”Ğ˜ĞĞ¡Ğ¢Ğ’Ğ•ĞĞĞ«Ğ™ Ğ¿ÑƒÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° (outline-RAG) =====
    def retrieve_via_outline(
        self,
        query: str,
        top_nodes: int = 12,
        top_pages: int = 6,
        top_chunks: int = 12,
        lazy_index_children: bool = False,
        restrict_to_dominant_space: bool = True,
    ) -> Dict[str, Any]:
        """
        1) Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ/Ñ‚Ğ¸Ñ‚Ğ»Ğ°Ğ¼ (Ğ±ĞµĞ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº Confluence),
        2) ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† ĞµÑ‰Ñ‘ Ğ½ĞµÑ‚ Ğ² Ñ‡Ğ°Ğ½ĞºĞ°Ñ… â€” Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµĞ¼ Ğ¸Ñ…,
        3) Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ² Ğ² Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ñ…,
        4) ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ â†’ Ğ¾Ñ‚Ğ²ĞµÑ‚.
        """
        candidate_pages: List[str] = search_outline(query, top_nodes=top_nodes, top_pages=top_pages) or []
        candidate_pages = [pid for pid in dict.fromkeys(candidate_pages) if pid]
        print(f"ğŸ” Outline â†’ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {len(candidate_pages)} â†’ {candidate_pages}")

        if not candidate_pages:
            return {"answer": "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (Ğ¿Ğ¾ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ).", "sources": []}

        # --- Ğ¡ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ¾ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ space, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ñ‚Ğ°Ñ‰Ğ¸Ñ‚ÑŒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ ---
        if restrict_to_dominant_space and len(candidate_pages) > 1:
            from collections import Counter
            page2space: Dict[str, str] = {}
            for pid in candidate_pages:
                p = pages_db.get_page(pid)
                if p and p.get("project_id"):
                    page2space[pid] = p["project_id"]

            counts = Counter(page2space.get(pid, "?") for pid in candidate_pages)
            counts.pop("?", None)
            if counts:
                dom_space, _ = counts.most_common(1)[0]
                filtered = [pid for pid in candidate_pages if page2space.get(pid) == dom_space]
                if filtered:
                    print(f"ğŸ“¦ Ğ”Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ space: {dom_space} â†’ {len(filtered)}/{len(candidate_pages)} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†")
                    candidate_pages = filtered

        # on-demand Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ (ĞµÑĞ»Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ ĞµÑ‰Ñ‘ Ğ½ĞµÑ‚ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²)
        for pid in candidate_pages:
            if not chunks_db.has_chunks(pid):
                self.index_page(pid, include_children=lazy_index_children)

        qv = np.asarray(get_embeddings([query])[0], dtype=np.float32)
        hits = query_similar_in_pages(qv, page_ids=candidate_pages, top_k=top_chunks) or []
        if not hits:
            return {"answer": "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (Ğ¿Ğ¾ÑĞ»Ğµ ÑÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ).", "sources": []}

        context_parts: List[str] = []
        sources: List[Dict[str, Any]] = []
        seen_pages = set()

        for h in hits:
            page_id = h.get("page_id")
            txt = h.get("document", "")
            if not txt:
                continue
            context_parts.append(f"[Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {page_id}]\n{txt}")

            if page_id and page_id not in seen_pages:
                meta = pages_db.get_page(page_id) or {}
                title = meta.get("title") or "â“ Unknown"
                url = (meta.get("url") or "") or confluence_page_url(page_id)  # â† ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ· Ğ‘Ğ”, Ğ·Ğ°Ñ‚ĞµĞ¼ fallback
                sources.append({
                    "page_id": page_id,
                    "title": title,
                    "url": (url or "").strip(),
                })
                seen_pages.add(page_id)

        if not context_parts:
            return {"answer": "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° (ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½ĞµÑ‚).", "sources": list(sources)}

        context = "\n\n---\n\n".join(context_parts)
        prompt = (
            "Ğ¢Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑˆÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.\n"
            "Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ â€” Ñ‚Ğ°Ğº Ğ¸ ÑĞºĞ°Ğ¶Ğ¸.\n\n"
            f"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:\n{context}\n\n"
            f"Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {query}\n"
            "ĞÑ‚Ğ²ĞµÑ‚:"
        )
        answer = (generate_answer(prompt) or "").strip()

        # Fallback, ĞµÑĞ»Ğ¸ LLM Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¾ Ğ¿ÑƒÑÑ‚Ğ¾ â€” ĞºÑ€Ğ°Ñ‚ĞºĞ°Ñ Ğ²Ñ‹Ğ¶Ğ¸Ğ¼ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
        if not answer:
            preview = context.strip()
            if len(preview) > 1500:
                preview = preview[:1500] + "â€¦"
            answer = preview if preview else "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ."
        
        print("SOURCES_DEBUG:", sources)
        return {"answer": answer, "sources": list(sources)}


# ========= Ğ¡Ğ‘ĞĞ  ĞĞ“Ğ›ĞĞ’Ğ›Ğ•ĞĞ˜Ğ¯/HEADINGS (Ğ‘Ğ•Ğ— Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ) =========

class OutlinePipeline:
    def __init__(self, domain: str, email: str, api_token: str):
        self.cf = ConfluenceConnector(domain, email, api_token)

    def build_outline(self, per_space_limit: int = 5000):
        """
        Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ´ĞµÑ€ĞµĞ²Ğ¾ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ (page title + headings) Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ´Ğ»Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.
        ĞšĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† ĞĞ• Ñ‚ÑĞ½ĞµĞ¼.
        """
        spaces = self.cf.get_spaces()
        print(f"ğŸ”„ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²: {len(spaces)}")
        nodes_counter = 0

        for sp in spaces:
            sid = sp["id"]
            sname = sp.get("name") or sp.get("key") or sid
            pages_db.insert_project(sid, sname)

            # Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ¼ĞµÑ‚Ğ°-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ€Ğ°Ğ·Ğ¾Ğ¼
            pages_meta = []
            try:
                pages_meta = self.cf.get_pages_meta(sid, limit=per_space_limit) or []
            except Exception as e:
                print(f"âš ï¸ get_pages_meta failed for space {sid}: {e}")
                pages_meta = []

            if not pages_meta:
                # ĞµÑĞ»Ğ¸ ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€ Ğ½Ğµ Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ meta â€” Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°ĞµĞ¼; outline Ğ¸Ğ· title Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¶Ğµ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
                pages_meta = pages_db.list_pages_in_space(sid) or []

            for p in pages_meta:
                pid = p["id"]
                title = p.get("title") or pid

                # Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ğ° Ğ² pages_db (Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°)
                _ins_page_meta(
                    page_id=pid,
                    project_id=sid,
                    title=title,
                    parent_id=p.get("parent_id"),
                    version=str(p.get("version") or ""),
                    last_modified=p.get("last_modified"),
                )

                # ÑƒĞ·ĞµĞ» Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ = ÑĞ°Ğ¼ title ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ (level 0)
                node_page = uuid4_str()
                pages_db.upsert_outline_node(
                    id=node_page, project_id=sid, page_id=pid, heading_id=None,
                    title=title, level=0, parent_id=None, path=title, updated_at=now_ts()
                )
                vec = get_embeddings([title])[0]
                pages_db.put_embedding(
                    object_type="outline_title", object_id=node_page, model_name="default",
                    content_sha256=sha256_text(title),
                    vector=np.asarray(vec, dtype=np.float32), created_at=now_ts()
                )
                nodes_counter += 1

                # Ğ´Ğ¾Ñ‡ĞµÑ€Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
                heads = []
                try:
                    heads = self.cf.get_page_headings(pid) or []
                except Exception as e:
                    print(f"âš ï¸ get_page_headings failed for page {pid}: {e}")
                    heads = []

                parent_map = {0: node_page}
                for h in heads:
                    level = int(h.get("level", 1))
                    h_text = h.get("text") or ""
                    if not h_text:
                        continue
                    h_node = uuid4_str()
                    parent_id = parent_map.get(level - 1, node_page)
                    parent_path = pages_db.get_outline_path(parent_id) or title
                    path = f"{parent_path} > {h_text}"

                    pages_db.upsert_outline_node(
                        id=h_node, project_id=sid, page_id=pid, heading_id=h.get("id"),
                        title=h_text, level=level, parent_id=parent_id, path=path, updated_at=now_ts()
                    )
                    vec = get_embeddings([path])[0]
                    pages_db.put_embedding(
                        object_type="outline_title", object_id=h_node, model_name="default",
                        content_sha256=sha256_text(path),
                        vector=np.asarray(vec, dtype=np.float32), created_at=now_ts()
                    )
                    parent_map[level] = h_node
                    nodes_counter += 1

        print(f"ğŸ§± outline built: nodes_inserted_or_updated={nodes_counter}")
